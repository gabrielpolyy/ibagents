# IBKR-Web-MCP Application Context

## Overview

IBKR-Web-MCP is a comprehensive trading and portfolio management application that provides AI-powered tools for interacting with Interactive Brokers (IB) Client Portal Web API. The application serves as a bridge between AI assistants and IB's trading platform through the Model Context Protocol (MCP), enabling intelligent portfolio management, market analysis, and automated trading operations.

## Core Architecture

### MCP Server (`server.py`)
- **FastMCP-based server** running on port 8000 that exposes IB trading functionality as AI tools
- **RESTful API endpoints** for tool discovery (`/tools/list`) and execution (`/tools/call`)
- **Comprehensive tool categorization** across 6 main functional areas
- **Error handling and validation** for all trading operations

### IB Client Library (`ib_client/`)
A modular Python client library providing:
- **Portfolio Management**: Account summaries, positions, P&L tracking, ledger operations
- **Market Data**: Real-time snapshots, historical OHLCV data, order book depth
- **Trading Operations**: Order placement, cancellation, what-if analysis, live order monitoring
- **Market Scanning**: Top gainers/losers, most active stocks, custom scanner codes
- **Core Infrastructure**: Contract resolution, authentication, API communication

### Tool Discovery (`mcp_tools.py`)
- **Diagnostic utility** for exploring available MCP tools
- **Categorized tool listing** with descriptions and usage examples
- **Development aid** for understanding API capabilities

## Tool Categories

### ğŸ“Š Reference & Contract Tools
- Contract resolution and symbol lookup
- Exchange information and trading parameters

### ğŸ’¼ Portfolio & Account Tools  
- Multi-account support and account selection
- Real-time portfolio summaries and net liquidation values
- Position tracking and performance monitoring
- Cash and FX balance management

### ğŸ“ˆ Market Data Tools
- Real-time market snapshots with bid/ask/last pricing
- Historical data retrieval with customizable timeframes
- Level 2 order book data for market depth analysis

### ğŸ“‹ Trading Tools
- Pre-trade risk analysis with what-if scenarios
- Order placement with multiple order types (Market, Limit, Stop)
- Live order monitoring and management
- Order cancellation and modification

### ğŸ” Scanner Tools
- Market screening for top gainers, losers, and most active stocks
- Custom scanner code execution
- Configurable result limits and geographic filters

### ğŸ“Š Analysis Tools
- Comprehensive portfolio health assessments
- Individual stock analysis with risk metrics
- Portfolio allocation and cash management insights

## Key Features

### AI-First Design
- **Tool-based architecture** enabling AI assistants to perform complex trading operations
- **Natural language interfaces** for portfolio management and market analysis
- **Contextual responses** with relevant financial data and risk assessments

### Risk Management
- **What-if analysis** before order execution
- **Portfolio health monitoring** with real-time metrics
- **Position sizing calculations** relative to available funds

### Real-Time Integration
- **Live market data** streaming from IB Client Portal
- **Real-time P&L tracking** across asset classes
- **Dynamic order book** and market depth information

### Multi-Account Support
- **Account discovery** and automatic primary account selection
- **Cross-account** order and position management
- **Account-specific** risk and performance analytics

## Technical Requirements

- **Interactive Brokers Client Portal Gateway** (port 8765) for market connectivity
- **Python 3.8+** with async/await support
- **FastMCP framework** for tool server implementation
- **Authenticated IB account** with API trading permissions

## Use Cases

- **Algorithmic Trading**: AI-driven order placement and portfolio rebalancing
- **Risk Management**: Automated portfolio health monitoring and alerts
- **Market Research**: AI-powered stock screening and analysis
- **Portfolio Optimization**: Intelligent asset allocation and position sizing
- **Trading Education**: Interactive exploration of market data and trading concepts

â”Œâ”€â”€ START (cron trigger @14:00) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Market & Alt-Data Ingest                                  â”‚
â”‚ 2. Signal Computation  â”€â”                                    â”‚
â”‚ 3. Portfolio Snapshot   â”‚â”€â”€â–º 4. Target Vector Optimisation â”€â”€â”¤
â”‚                         â””â”€â”€â–º 5. News & Sentiment Overlay     â”‚
â”‚ 6. Risk & Compliance Gate                                    â”‚
â”‚ 7. What-If Simulation                                        â”‚
â”‚ 8. Order List Generation & (optional) Auto-Submit            â”‚
â”‚ 9. Decision Journal Append                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Hey Gabi! Below is a pragmatic â€œoperating manualâ€ for running your IBKR-Web-MCP agent every day, end-to-endâ€”from cron-kickoff to audit-grade logging. Steal whatever chunks fit your workflow.

â¸»

0 Â· Clock & Environment

Location	Local clock	Typical U.S. cash-session (NYSE / NASDAQ)	Comment
Bucharest	UTC +3	16:30 â†’ 23:00 EEST	Youâ€™ll pre-compute signals during your afternoon coffee and still have a few hours of sunlight.

All times below are Bucharest (EEST). Adjust to target exchanges as needed.

â¸»

1 Â· Daily Scheduler

Option A â€” pure cron

Put the one-liner agent call in /etc/cron.d/alpha_agent:

# â”Œ minute (0-59)
# â”‚ â”Œ hour (0-23, EEST)
# â”‚ â”‚  â”Œ dom
# â”‚ â”‚  â”‚  â”Œ month
# â”‚ â”‚  â”‚  â”‚  â”Œ dow (0-Sun)
# â”‚ â”‚  â”‚  â”‚  â”‚
  00  14  *  *  1-5  gabi   /opt/alpha/bin/run_agent.sh >> /var/log/alpha_agent.log 2>&1

Runs 14:00 daily (â‰ˆ 08:00 NY).

Option B â€” Airflow / Prefect / Dagster

Use an orchestrator if you want retries, backfills, alerting, and parameterized back-tests. Treat each bold heading in Â§2 as a task.

â¸»

2 Â· Pipeline Stages (high-level DAG)

â”Œâ”€â”€ START (cron trigger @14:00) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Market & Alt-Data Ingest                                  â”‚
â”‚ 2. Signal Computation  â”€â”                                    â”‚
â”‚ 3. Portfolio Snapshot   â”‚â”€â”€â–º 4. Target Vector Optimisation â”€â”€â”¤
â”‚                         â””â”€â”€â–º 5. News & Sentiment Overlay     â”‚
â”‚ 6. Risk & Compliance Gate                                    â”‚
â”‚ 7. What-If Simulation                                        â”‚
â”‚ 8. Order List Generation & (optional) Auto-Submit            â”‚
â”‚ 9. Decision Journal Append                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Below is the how for each node.

â¸»

1. Market & Alt-Data Ingest  (14:00-14:05)

Feed	Tool / API	Granularity	Latency OK?
IBKR snapshots	MarketData.bulk_quote()	live	seconds
Historical bars	MarketData.history()	1 m-1 d	seconds
Fundamentals	EOD fundamental dump (e.g. Financial Modeling Prep)	1Ã— day	minutes
News sentiment	GDELT, RavenPack, or open-source FinBERT run on RSS	minute	minutes

Write all raw payloads to /data/staging/YYYY-MM-DD/â€¦ then emit a parquet â€œfeature matrixâ€ (one row per symbol) into /data/features/.

â¸»

2. Signal Computation  (14:05-14:15)
	â€¢	Technical block:
	â€¢	moving averages crossover, ATR breakout, RSI extremes, etc.â€”vectorised in numpy/pandas or TA-Lib bindings.
	â€¢	Fundamental block:
	â€¢	Piotroski F-Score, forward P/E deviation vs sector, YoY revenue delta > 15 %, etc.
	â€¢	Sentiment block:
	â€¢	z-score of headline polarity past 24 h, Reddit r/wallstreetbets post spike, Twitter volume anomaly.

Normalize each block to -1 â€¦ +1, blend with learned weights, output alpha_score.

â¸»

3. Portfolio Snapshot  (14:05-14:06, parallel)

from ib_client import Portfolio
current_pos = Portfolio.snapshot()   # JSON: {symbol: {qty, market_value, â€¦}}

Persist to /data/positions/YYYY-MM-DD.json.

â¸»

4. Target Vector Optimisation  (14:15-14:18)

from optimiser import mean_variance_cvar
targets = mean_variance_cvar(
    universe      = feature_df.symbol,
    alpha_scores  = feature_df.alpha_score,
    risk_model    = 'shrunk_cov',
    max_gross     = 1.2,
    max_single    = 0.12,
    cash_buffer   = 0.05)

Returns {symbol: target_weight} plus risk metrics.

â¸»

5. News & Sentiment Overlay  (14:18-14:20)

Hard overridesâ€”examples:

if sentiment["NVDA"] < -2.5:  targets.pop("NVDA", None)
if breaking_news_contains("FDA ban") and "PM" in targets: targets["PM"] = 0


â¸»

6. Risk & Compliance Gate  (14:20-14:22)
	â€¢	Run risk.check_constraints(current_pos, targets):
	â€¢	Î” Gross Exposure, VAR < threshold
	â€¢	Concentration < 12 %
	â€¢	Liquidity: shares â‰¤ 10 % avg-daily-vol
	â€¢	If fail â†’ export â€œrecommendation onlyâ€ report; abort execution path.

â¸»

7. What-If Simulation  (14:22-14:25)

For each candidate order produced by portfolio.diff_to_orders(targets):

sim = TradingTools.what_if(order)
if sim.maintenance_margin_exceeded:
    flag, remove, or size-down by 20 %


â¸»

8. Order List Generation & (optionally) Auto-Submit  (14:25)

orders = portfolio.diff_to_orders(targets, force_even_lots=True)
if AUTO_MODE:
    TradingTools.place_orders(orders)
else:
    save_json("/reports/order_ticket_2025-06-09.json", orders)
    send_slack("Orders ready for review: ...")


â¸»

9. Decision Journal Append  (14:25-14:26)

Append a row to /data/decision_log/ (parquet + sha256 file links):

journal.log(
    timestamp        = now_utc,
    hash_targets     = sha256(targets),
    hash_orders      = sha256(orders),
    top_drivers      = feature_df.nlargest(5, 'alpha_score')[['symbol','alpha_score']],
    expected_return  = optimiser.ex_ante_return,
    expected_vol     = optimiser.ex_ante_vol,
    constraints_ok   = True)


â¸»

3 Â· Monitoring & Alerting
	â€¢	Prometheus + Grafana âœ CPU, latency, failed tasks.
	â€¢	Slack / Email âœ
	â€¢	14:30 ping: â€œOrders queued / executed.â€
	â€¢	Exception notifier: risk-gate fail, IBKR reject, cron exitâ‰ 0.

â¸»

4 Â· Back-Testing & Continuous Calibration
	1.	Schedule a weekly Airflow DAG to replay the entire pipeline on the last 6 months of data.
	2.	Write Sharpe, max-drawdown, turnover into /metrics/rolling_backtest.csv.
	3.	Automatically bump signal weights if the multi-objective optimiser says so (optional but spicy).

â¸»

5 Â· Security & Ops Checklist
	â€¢	Store IBKR gateway key in HashiCorp Vault or AWS Secrets Manager.
	â€¢	Rotate API tokens monthly; pipeline should read them at runtime.
	â€¢	Use a dedicated IBKR paper account for staging â†’ promote configs to live after X green days.
	â€¢	Version-control everything (infra code, signal notebooks, configs) in Git; tag Y-m-d releases.

â¸»

TL;DR
	1.	Kick off at 14:00 EEST with cron (or orchestrator).
	2.	Ingest â†’ score â†’ optimise â†’ risk-gate in ~25 min.
	3.	Produce target-vector + atomic order list, optionally auto-submit via IBKR MCP.
	4.	Log every artefact (raw data, orders, reasons) for traceability.
	5.	Alert on success/failure; rerun intraday if you crave higher frequency.

Stick to that skeleton and youâ€™ll have a daily machine that thinks, acts, andâ€”cruciallyâ€”remembers why.

Ping me any time you want to zoom in on a specific stage, add an NLP headline classifier, or wire up federated back-testing. Have fun building, maestro! ğŸ› ï¸


Sure thing, Gabiâ€”letâ€™s zoom in on that â€œMarket & Alt-Data Ingestâ€ block and translate it from engineer-speak to plain English:

Feed	What youâ€™re really grabbing	Why you need it	How fresh it is
IBKR snapshots	The latest bid/ask price, last trade, volume, etc. for every symbol you care about.	Tells you â€œwhatâ€™s happening this very secondâ€ so you can size orders, set good limit prices, and spot sudden jumps.	Arrives in seconds.
Historical bars	Candlestick data (open, high, low, close, volume) over the last day, week, monthâ€”whatever timeframe you ask for.	Fuels technical indicators like moving averages, RSI, breakouts, etc. You need a history to judge whether todayâ€™s move is unusual.	Also seconds (itâ€™s just downloading the past).
Fundamentals	End-of-day snapshots of balance-sheet stuff: P/E, revenue growth, EPS surprises, debt ratios, â€¦	Lets you know if the company is cheap, growing, over-leveraged, etc. Good for fundamental alpha models and risk checks.	Updated once per day after markets closeâ€”minutes of delay is fine because fundamentals donâ€™t change intraday.
News sentiment	Headlines + social chatter run through an NLP model that scores each symbol from very negative (-) to very positive (+).	Catches narrative shiftsâ€”e.g., â€œFDA rejectionâ€ or â€œearnings beatâ€â€”that price data alone hasnâ€™t digested yet.	A few minutes behind real time (RSS / API poll + model run).


â¸»

Soâ€¦ what does â€œingestâ€ actually look like?
	1.	Decide on your watch-list.
	â€¢	Might be the entire S&P 500, a curated factor universe, or simply â€œwhatever I hold + todayâ€™s top movers.â€
	2.	Pull the four feeds above in parallel.

prices_live   = MarketData.bulk_quote(watchlist)      # IBKR snapshots
prices_hist   = MarketData.history(watchlist, "1d")   # historical bars
fundamentals  = fmp_api.get_fundamentals(watchlist)   # or similar
sentiment     = news_pipe.score_sentiment(watchlist)  # FinBERT, GDELT, etc.


	3.	Stick all of it into one table (one row per symbol, many columns of features).
Thatâ€™s the feature matrix your signal engine works on.

â¸»

Macro vs. Micro vs. Top Moversâ€”how to sequence?
	1.	Macro sweep (index + rates + commodities).
	â€¢	Pull 10-year Treasury yield, EUR/USD, crude oil, VIX, SPY, QQQ.
	â€¢	Build global signals like â€œrisk-on vs risk-off,â€ â€œyield-curve steepening,â€ etc.
	â€¢	These become environment featuresâ€”e.g., risk-off penalty or sector tilts.
	2.	Micro fundamentals & sentiment (per-symbol).
	â€¢	Stick to your universe; evaluate each stockâ€™s value/growth quality + todayâ€™s news tone.
	3.	Top-mover scan (optional fast-lane).
	â€¢	Use IBKR Scanner Tools: â€œTOP%GAINâ€ / â€œTOP%LOSERâ€ / â€œHOT_BY_VOLUME.â€
	â€¢	Any symbol that moves, say, > 5 % on heavy volume but isnâ€™t already in your watch-list gets temporarily added for todayâ€™s run.
	â€¢	This stops you missing sudden opportunities in off-radar names.

â¸»

A 3-minute mental picture

14:00 â€” Pull macro gauges (10-year, VIX, index futures)
14:01 â€” Pull live quotes + 30-day candles for ~700 symbols
14:02 â€” Pull yesterdayâ€™s fundamentals dump (CSV / API)
14:03 â€” Pull news headlines last 24 h â†’ run sentiment model
14:04 â€” Merge â†’ big DataFrame â†’ onward to â€œSignal Computationâ€

Thatâ€™s really it: four data buckets, one merge, done. The rest of the pipeline (technicals, fundamentals, sentiment scoring, optimisation) works off that unified snapshot.

Does that clear up what youâ€™re ingesting and why? Let me know if you want code snippets for a specific feed or deeper talk on, say, the sentiment NLP set-up.

